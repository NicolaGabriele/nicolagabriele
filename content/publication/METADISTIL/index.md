---
title: "A Parameter-Efficient Approach to Distilling Large Language Models via Meta-Learning"
date: 2025-09-23
publishDate: 2025-09-23
authors: ["Riccardo Cantini","Nicola Gabriele", "Alessio Orsino"]
abstract: "The edge-cloud continuum enables distributed machine learn-
ing by leveraging the complementary strengths of edge devices and cen-
tralized cloud resources. Federated Learning (FL) has emerged as a key

paradigm in this context, allowing collaborative model training across

multiple parties without sharing raw data, thus preserving privacy, re-
ducing communication costs, and supporting compliance with data pro-
tection regulations. However, orchestrating FL workflows across hetero-
geneous edge-cloud environments introduces significant challenges re-
lated to task coordination, resource management, and scalability. In this

paper, we propose using the Colony framework to address these chal-
lenges through a task-based approach to FL. Colony allows develop-
ers to define an FL workflow as parallel tasks automatically scheduled

across heterogeneous resources. We show how this task-based model sup-
ports core FL operations—such as local training, model aggregation, and

synchronization—within a unified execution framework. Experiments on
a medical imaging use case demonstrate that Colony enables scalable
and efficient orchestration of FL tasks across heterogeneous environments
while ensuring that sensitive data remain local. These results highlight
the applicability and advantages of task-based programming models for
privacy-preserving machine learning across the compute continuum."
featured: false
publication: "HeteroPar 2025, 23rd International Workshop"
# url_pdf:
# doi:


# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
image:
  caption: ""
  focal_point: ""
  preview_only: false


tags: ["Edge-Cloud Continuum", "Federated Learning", "Privacy-Preserving Machine Learning", "Task-Based Programmings"]

---
