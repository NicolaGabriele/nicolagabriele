---
title: "Is Reasoning What You Need to Mitigate Bias? \\ A Study of Adversarial Robustness to Bias Elicitation in Large Reasoning Models"

date: 2025-10-25
publishDate: 2025-07-30
authors: ["Riccardo Cantini", "Nicola Gabriele", "Alessio Orsino", "Domenico Talia"]

abstract: "Large Reasoning Models (LRMs) have gained prominence for their ability to perform complex, multi-step reasoning tasks, often through mechanisms such as 
chain-of-thought (CoT) prompting or fine-tuned reasoning traces. While these capabilities promise improved reliability and alignment, it remains unclear whether they 
lead to enhanced robustness against social biases. In this work, we apply the CLEAR-Bias benchmark, originally designed for Large Language Models (LLMs), to investigate 
the adversarial robustness of LRMs to bias elicitation. Our evaluation addresses two key questions: $(i)$ how the introduction of reasoning capabilities affects model 
fairness and robustness, and $(ii)$ whether models fine-tuned for reasoning exhibit greater safety than those that rely on CoT prompting at inference time. 
We systematically probe a range of state-of-the-art LRMs using a multi-task approach across sociocultural dimensions, quantify robustness via automated safety scoring 
with an LLM-as-a-Judge paradigm, and employ jailbreak techniques to assess the integrity of built-in safety mechanisms. Our findings reveal that the impact of reasoning 
capabilities on safety is highly model-dependent, potentially leading to either improvements or degradations. Moreover, despite their enhanced reasoning abilities, LRMs 
remain vulnerable to adversarial bias elicitation, suggesting that reasoning alone may be insufficient for effective bias mitigation."

featured: false
publication: "HeteroPar 2025, 23rd International Workshop"
# url_pdf:
# doi:


# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
image:
  caption: ""
  focal_point: ""
  preview_only: false


tags: ["Large Reasoning Models", "Large Language Models", "Bias", "Stereotype", "Jailbreak", "Adversarial Robustness", "Sustainable AI"]

---
